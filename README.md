# ğŸš€ Sudo_NLP

A hands-on journey into Natural Language Processing (NLP) â€” my personal space for learning and experimenting with Natural Language Processing (NLP).

This repository is more than just code: itâ€™s a living journal of my progress as I explore how machines understand and process human language.

# ğŸ§© What youâ€™ll find here

ğŸ““ Jupyter Notebooks documenting my step-by-step experiments

ğŸ—‚ï¸ Learning Notes & Resources â€” summaries, references, and takeaways saved in text/Docx files to track my progress and key insights

# ğŸ¯ Goals of this journey

Build a strong foundation in NLP concepts

Apply concepts through practical coding exercises

Create a reference I can return to as my knowledge grows

Share the journey with others learning NLP

# ğŸ“… Schedule

| ğŸ§© Module                                | ğŸ“– Content                                                                                               | ğŸ“š Resources                     | Status |
| ---------------------------------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------- | -------- |
| **Text preprocessing**                   | Tokenizer <br> Vectorization <br> Stopwords <br> Lemmatization <br> Normalization                        | NLTK â€“ Text Preprocessing        | âœ…        |
| **Text feature extraction**              | Bag of Words <br> TFIDF <br> Unigram <br> Bigram                                                         | Scikit-learn: Feature Extraction | âœ…        |
| **Text embeddings**                      | Word2Vec <br> Avg Word2Vec <br> Doc2Vec                                                                  | Gensim Word2Vec                  | âœ…        |
| **Machine Learning for NLP**             | SVM <br> Naive Bayes                                                                                     | Scikit-learn Classification      | âœ…        |
| **Deep Learning for NLP**                | Hidden layer <br> Perceptron <br> Gradient descent <br> Loss function                                    | Deep Learning Book, TensorFlow   | âœ…        |
| **Sequential Models**                    | ANN <br> RNN <br> LSTM <br> GRU                                                                          | Understanding LSTM Networks      |         |
| **Attention model**                      | Encoder-decoder <br> Self-attention                                                                      | *Attention Is All You Need*      |         |
| **Transformer**                          | Transformer                                                                                              | The Transformer Model            |         |
| **NghiÃªn cá»©u LLM**                       | GPT, Gemini, Claude <br> LLaMA, MPT, Bloom <br> Metrics: GLUE, MMLU                                      | OpenAI Docs, Gemini Docs         |        |
| **RAG & Agents (LangChain, LlamaIndex)** | RAG concepts <br> Vector DB <br> LangChain (Chains, Tools, Agents) <br> LlamaIndex (retrievers, engines) | LangChain Docs, LlamaIndex Docs  |        |

# ğŸŒ± Current Status

ğŸ”§ Early stages â€” setting up preprocessing experiments and basic embeddings.
This README (and repo) will evolve as I learn more.

<p align="center"><i><b>â€œThe best way to learn is by doing â€” every experiment, every mistake, every iteration is a step closer to mastery.â€</b></i></p>
